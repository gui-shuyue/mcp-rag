import asyncio # Python çš„å¼‚æ­¥ç¼–ç¨‹æ ‡å‡†åº“ï¼Œæä¾›å¼‚æ­¥/ç­‰å¾…ï¼ˆasync/awaitï¼‰ç¼–ç¨‹æ”¯æŒ
import os
from mcp import Tool #å®šä¹‰å’Œç®¡ç† AI æ¨¡å‹å¯ä»¥ä½¿ç”¨çš„å·¥å…·
from openai import AsyncOpenAI #ä¸ OpenAI API è¿›è¡Œå¼‚æ­¥é€šä¿¡
from dataclasses import dataclass, field #è£…é¥°å™¨ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆç±»çš„ç‰¹æ®Šæ–¹æ³•
                                         #fieldç”¨äºè‡ªå®šä¹‰ dataclass å­—æ®µçš„è¡Œä¸º
                                         #è‡ªåŠ¨ç”Ÿæˆ __init__ã€__repr__ ç­‰æ–¹æ³•
                                         # ç®€åŒ–ç±»çš„å®šä¹‰
                                         # æä¾›ç±»å‹æç¤ºæ”¯æŒ
from openai.types import FunctionDefinition #å®šä¹‰ä¸ OpenAI API äº¤äº’æ—¶ä½¿ç”¨çš„å‡½æ•°
from openai.types.chat import(
    ChatCompletionMessageParam,
    ChatCompletionToolParam,
)
import dotenv
from pydantic import BaseModel #TODO
from rich import print as rprint

from utils import pretty

dotenv.load_dotenv() #åŠ è½½ç¯å¢ƒå˜é‡ï¼Œä» .env æ–‡ä»¶ä¸­è¯»å–é…ç½®

class ToolCallFunction(BaseModel):
    name: str = ""
    arguments: str = ""

class ToolCall(BaseModel):
    id: str = ""
    function: ToolCallFunction = ToolCallFunction()

class ChatOpenAIChatResponse(BaseModel):
    content: str = ""
    tool_calls: list[ToolCall] = []


@dataclass
class AsyncChatOpenAI:
    model: str
    messages:list[ChatCompletionMessageParam] = field(default_factory=list) #databaseä¸­çš„å¯å˜å‚æ•°ï¼Œç”¨feildä½¿å…¶ç‹¬ç«‹äºä¸€ä¸ªå®ä¾‹
    tools: list[Tool] = field(default_factory=list)

    system_prompt: str = ""
    context: str = ""

    llm: AsyncOpenAI = field(init=False)
    # è¯¥å­—æ®µä¸ä¼šåœ¨ dataclass çš„è‡ªåŠ¨ç”Ÿæˆçš„ __init__ æ–¹æ³•ä¸­åˆå§‹åŒ–ã€‚

    def __post_init__(self):
        self.llm = AsyncOpenAI(
            api_key=os.environ.get("OPENAI_API_KEY"),
            base_url=os.environ.get("OPENAI_BASE_URL"),
        )
        if self.system_prompt:
            self.messages.insert(0, {"role": "system", "content": self.system_prompt})
        if self.context:
            self.messages.append({"role": "user", "content": self.context})

    async def chat(self, prompt: str = "", print_llm_output: bool = True) -> ChatOpenAIChatResponse:
        pretty.log_title("CHAT")
        if prompt:
            self.messages.append({"role": "user", "content": prompt})       
        streaming = await self.llm.chat.completions.create(             
            model=self.model,
            messages=self.messages,
            tools=self.getToolsDefinitions(),
            stream=True,
        )
# # å…¸å‹ chunk ç¤ºä¾‹
# {
#     "id": "chatcmpl-...",
#     "object": "chat.completion.chunk",
#     "created": 1234567890,
#     "model": "gpt-4",
#     "choices": [
#         {
#             "index": 0,
#             "delta": {
#                 "content": "Hello",  # éƒ¨åˆ†å†…å®¹
#                 "tool_calls": [...]   # å·¥å…·è°ƒç”¨ï¼ˆå¯é€‰ï¼‰
#             },
#             "finish_reason": None
#         }
#     ]
# }

        pretty.log_title("RESPONSE")
        content = ""
        tool_calls: list[ToolCall] = []
        printed_llm_output = False
        async for chunk in streaming:
            delta = chunk.choices[0].delta
            if delta.content:
                content += delta.content
                if print_llm_output:
                   print(delta.content, end="")
                   printed_llm_output = True

            if delta.tool_calls:
                for tool_call in delta.tool_calls:
                    if len(tool_calls) <= tool_call.index:
                        tool_calls.append(ToolCall())
                    current_call = tool_calls[tool_call.index]
                    if tool_call.id:
                        current_call.id = tool_call.id or ""
                    if tool_call.function:
                        current_call.function.name = tool_call.function.name or ""
                        current_call.function.arguments += (tool_call.function.arguments or "")
# # å…¸å‹ tool_call
# {
#     "index": 0,
#     "id": "call_123",
#     "function": {
#         "name": "get_weather",
#         "arguments": '{"city": "åŒ—äº¬"}'
#     }
# }
            
        
        if printed_llm_output:
            print()
        
        # ä¿®æ”¹åçš„ä»£ç ï¼š
        message = {
            "role": "assistant", 
            "content": content,
        }

        # ğŸ”§ åªåœ¨æœ‰å·¥å…·è°ƒç”¨æ—¶æ‰æ·»åŠ  tool_calls å­—æ®µ
        if tool_calls and len(tool_calls) > 0:
            # ğŸ”§ è¿‡æ»¤æ‰ç©ºçš„å·¥å…·è°ƒç”¨
            valid_tool_calls = [
                {
                    "type": "function",
                    "id": tool.id,
                    "function": {
                        "name": tool.function.name,
                        "arguments": tool.function.arguments,
                    }
                }
                for tool in tool_calls
                if tool.id and tool.function.name  # ğŸ”§ ç¡®ä¿ä¸ä¸ºç©º
            ]
            
            if valid_tool_calls:
                message["tool_calls"] = valid_tool_calls

        self.messages.append(message)
        
        return ChatOpenAIChatResponse(
            content=content,
            tool_calls=tool_calls,
        )
    
    
    
    
    def getToolsDefinitions(self) -> list[ChatCompletionToolParam]:
        return [
            ChatCompletionToolParam(
                type = "function",
                function=FunctionDefinition(
                    name=tool.name,
                    description=tool.description or "",
                    parameters=tool.inputSchema or {},
                ),
            )
            for tool in self.tools if tool.name and tool.name.strip()
        ]
    # ChatCompletionToolParam:
    # {
    # "type": "function",  # å·¥å…·ç±»å‹ï¼Œç›®å‰ä»…æ”¯æŒ "function"
    # "function": {
    #     "name": str,        # å‡½æ•°åç§°
    #     "description": str, # å‡½æ•°æè¿°
    #     "parameters": dict  # å‚æ•°å®šä¹‰ï¼ˆJSON Schemaï¼‰
    # }
    # }

    def append_tool_result(self, tool_call_id: str, tool_result: str):
        self.messages.append(
            {
                "role": "tool",
                "tool_call_id": tool_call_id,
                "content": tool_result,
            }
        )

async def example():
    llm = AsyncChatOpenAI(
        model="gpt-4o-mini",
    )
    chat_resp = await llm.chat(prompt="Hello")
    rprint(chat_resp)


if __name__ == "__main__":
    asyncio.run(example())